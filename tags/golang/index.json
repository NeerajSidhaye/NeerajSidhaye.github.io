[{"content":"This post is Prometheus hands on, where we will setup API monitoring using Prometheus, integration with Grafana for data visualization. The entire setup will be achieved using docker compose.\nIf you are new to Prometheus, I would recommend to read my post on What is Prometheus?\nWe will go through step by step as below\n Architecture\n  Exposing /metrics endpoint in GO API\n  Prometheus.yml\n  Docker compose\n  Running Docker Compose\n  Launching applications\ngo rest api, metrics endpoint, prometheus, grafana\n  Grafana configuration\n  Load test the api and monitor metrics on grafana\n  Show me the Code!\n Architecture  We are using docker compose and running 3 containers\na) golang\nb) prometheus\nc) grafana\nExposing /metrics endpoint in GO API Import prometheus client libraries For exposing metrics from our GO api, we will need to import golang client libraries for Prometheus.\nPrometheus has official client libraries for GO, Java or Scala, Python and Ruby.\nUnofficial client libraries are also available for other languages.\nHave a look here for list of Prometheus Client libraries\n1 2 3 4  \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34;   Creating Counter metrics and Middleware, registering counter metric to Prometheus 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  var totalRequest = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;http_request_total\u0026#34;, Help: \u0026#34;Number of get request\u0026#34;, }, []string{\u0026#34;path\u0026#34;}, ) func prometheusMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request){ next.ServeHTTP(w, r) totalRequest.With(prometheus.Labels{\u0026#34;path\u0026#34;:\u0026#34;/products\u0026#34;}).Inc() }) } func init(){ prometheus.Register(totalRequest) }   Registering Prometheus http handler to metrics endpoint 1 2 3 4 5 6 7  servMux := mux.NewRouter() servMux.Use(prometheusMiddleware) servMux.Path(\u0026#34;/metrics\u0026#34;).Handler(promhttp.Handler())   Prometheus.yml This is the configuration where we tell Prometheus to scarp the data from an endpoint. In this case, we are asking prometheus to pull metrics from golang application on /metrics end point and the IP address.\nThis is static configuration. Prometheus also supports service discovery pattern to find it\u0026rsquo;s targets for pulling the metrics.\n1 2 3 4 5 6 7 8 9  global:scrape_interval:15s scrape_configs:- job_name:golangmetrics_path:/metricsstatic_configs:- targets:[\u0026#34;192.168.99.100:8080\u0026#34;]  Docker compose Let\u0026rsquo;s create our docker compose file.\nThe file is self explanatory, the only catch is at line 16, while mounting prometheus volumes.\nAnd this is because, I am running docker on Windows 10 using Oracle Virtual VM.\nWith this docker setup, there is only one shared folder form your local host to virtual VM and it is c:\\\\users.\nIf we want to put prometheus.yml at some other location on local host then we have to make that folder sharable with Oracle VM box.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  version: \u0026#39;3.1\u0026#39;services: golang: container_name: golang build: context: ./ dockerfile: Dockerfile  restart: always ports: - 8080:7070 prometheus: image: prom/prometheus:latest container_name: prometheus volumes: - //c/Users/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml - prometheus_data:/prometheus command: - \u0026#39;--config.file=/etc/prometheus/prometheus.yml\u0026#39; - \u0026#39;--storage.tsdb.path=/prometheus\u0026#39; - \u0026#39;--web.console.libraries=/usr/share/prometheus/console_libraries\u0026#39; - \u0026#39;--web.console.templates=/usr/share/prometheus/consoles\u0026#39; ports: - 9090:9090 restart: always grafana: image: grafana/grafana:latest container_name: grafana volumes:  - grafana_data:/var/lib/grafana - ./grafana/provisioning:/etc/grafana/provisioning environment: - GF_AUTH_DISABLE_LOGIN_FORM=false - GF_AUTH_ANONYMOUS_ENABLED=false - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin ports: - 3000:3000 volumes:  grafana_data: prometheus_data:  Running Docker Compose starting docker compose 1  $ docker-compose up -d  -d is for running containers in detachable mode.\n verifying containers 1  $ docker ps  We have 3 containers running now\ngolang at 8080, prometheus at 9090 and grafana at 3000\n Launching applications Launch application with below urls.\nNote:- I am using \u0026ldquo;192.168.99.100\u0026rdquo; as host because that\u0026rsquo;s my Oracle VM ip address for docker.\n golang end points\n http://192.168.99.100:8080/products\nClick to see product API response \r\r\rhttp://192.168.99.100:8080/metrics\nClick to see metrics response \r\r\r Prometheus application\n This will show the metrics which prometheus data retrieval service is scrapping from golang application over /metrics\nhttp://192.168.99.100:9090/\nClick to see how data looks on Prom UI \r\r\r Grafana application\nhttp://192.168.99.100:3000/\nLogin with default user and password as admin.\n Grafana configuration Once we login to Grafana using admin and admin as user and password.\nCreate DataSource This is where, we tell Grafana to talk to Prometheus. After this configuration, grafana pull data from Prometheus htp server component using PromQL.\n Create dashboard This is where you will create dashboard and then panel.\nInside Panel you can add your promQL query and there are many other configuration to play around.\nPanel and it\u0026rsquo;s configuration\n\nLoad test the api and monitor metrics on grafana Let\u0026rsquo;s add load to the API, so that we can see real time graph in Grafana.\nI will use hey for adding load to my api. If you haven\u0026rsquo;t tried hey library, give a try. Hey is beautiful.\ndownload hey $ go get -u github.com/rakyll/hey to install hey\rVerify download $ which hey\rYou should see hey folder inside your $GO-PATH\\bin\\\nRunning the load hey -n 10 -c 2 -m GET -T \u0026ldquo;application/json\u0026rdquo; http://192.168.99.100:8080/products\n-n = Number of requests to run. -c = Number of workers to run concurrently\n-m = HTTP method\n-T = Content-type\nGrafana dashboard after load  Show me the Code! Go API with Prometheus and grafana integration using Docker Compose\n","description":"API monitoring using Prometheus and integration with Grafana for data visualization","id":3,"section":"posts","tags":["API","API Monitoring","Prometheus, Grafana","GoLang"],"title":"GO API monitoring using Prometheus, and Grafana Integration using Docker Compose","uri":"https://neerajsidhaye.gibhub.io/posts/api/monitoring/prometheus-handson/"},{"content":"This is the first post on Prometheus series. If you already know basics of Prometheus, then please read my post on Prometheus Hands-On Go api monitoring using Prometheus and Grafana Integration using Docker Compose\nIn this post, we will look at\nWhat is Prometheus?\nPrometheus Architecture\nPrometheus Metrics\nWhere does it fit best?\nWhere does it NOT Fit?\nKey Highlights\nWhat is Prometheus? This is an open-source monitoring and alerting system.\nPrometheus can be used to monitor highly dynamic container environments like Kubernetes, Docker Swarm or we can also use it as a traditional non-container application.\nTargets can be configured with service discovery pattern or static configuration is also supported.\nHey but what is target here? : Target is our application which expose an end point /metrics by using Prometheus client libraries and this is the end point where Prometheus can pull the data over http. So, our application is a target for Prometheus.\nPush mechanism is also supported for a special cases like short lived scheduled jobs and this is when PushGateway component of Prometheus comes into play.\n This Pull mechanism is one of the key differentiator of Prometheus from other monitoring systems like AWS CloudWatch, Nagios, NewRelic etc which installs a daemon (an additional step) in each service and pushes the metrics from service to centralized server.\n Stores metrics in a time series format with key/value pair.\nPromQL fetches the metrics and aggregated data from time series database via HTTP server.\nPrometheus Architecture  Prometheus consists of various components but at it\u0026rsquo;s core there are 3 main components and other are optional components.\nEach Prometheus server has these 3 components.\nData Retrieval Worker This is responsible for scraping metrics from external sources/targets and pushing into the time series database.\nTime series database Time series means, changes are recorded over time. For an api, it could be number of request per minute, for database, it may be number of active connections on a given time.\nPrometheus stores all metics data on a local on-disk time series database. It can also optionally integrates with remote storage systems.\nThis uses multi-dimensional data model. Time series defined by metric name and set of key/value dimensions.\nHttpServer This component accepts queries from clients ( data visualization clients like Granfa, Prometheus UI ) or alerts managers and then fetch metrics from prometheus database.\n Additional components\n Pushgateway This comes into play when we want to monitor short lived jobs, like batch job etc. These short lived jobs then push it\u0026rsquo;s metrics to Pushgateway, which then intern transfer these metrics to the Prometheus data retrieval worker.\nAlert Manager This component receive the messages from HTTP Server component and sends notification to it\u0026rsquo;s clients, which could be PagerDuty, Slack, any Email or SMS tools etc.\nService Discovery Prometheus is designed to quickly up and running with a very basic setup and also supports dynamic configuration for looking up targets in a container based environments like Kubernetes with service discovery pattern.\nPrometheus Metrics Prometheus has got very simple to understand text-format metrics\nThere are currently 4 types of metrics supported\n Counter Counter represents a cummulative value. It can increase, reset to zero. Use this in a situations when you want know how many times a x has happened. For example, Number of request served by an api, number of errors, number of task completed.\nMost counters are therefore named using the _total suffix e.g. http_requests_total.\nImportant - do not use counter for things which are decreasing. For example - number of currently running process. We should be using Gauge for this case.\nGauge Its a value that may go up or down. For example, current memory usage or also can be used for count which can go down like number of concurrent requests.\nHistogram This is a observed metrics shared into distinct bucket. If you want to track something \u0026ldquo;how long this took\u0026rdquo; or \u0026ldquo;how big something was\u0026rdquo;, then use Histogram.\nSummary Similar to histogram and also provides total count of observations.\nWhere does it fit best? It is ideal in highly dynamic systems such as microservices running in a cloud environment.\nWhere does it NOT fit? It may not be best fit where we need 100% accuracy like real time billing system.In such a case the specific billing function should be addressed with an alternative, but Prometheus may still be the right tool for monitoring the other application and infrastructure functions.\nKey Highlights üîÜ Prometheus is recognized as a Cloud Native Computing Foundation member project.\nüîÜ Most of the Prometheus components are written in GO.\nüîÜ Recording any purely numeric time series.\nüîÜ Fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures.\nüîÜ Support for multi-dimensional data collection in a world of MicroServices.\nüîÜ Strong querying mechanism.\nüîÜ Each Prometheus server is standalone, not depending on network storage or other remote services.\n","description":"An open-source monitoring and alerting system to enlighten us regarding how our applications are performing!","id":4,"section":"posts","tags":["API","API Monitoring","Prometheus"],"title":"What is Prometheus?","uri":"https://neerajsidhaye.gibhub.io/posts/api/monitoring/prometheus-concept/"},{"content":"In this post, we will be creating, deploying and invoking a aws lambda function in Go.\n We will be using AWS CLI throughout this post for\n Creating role ( aws iam create-role ) Attaching policy to the role ( aws iam attach-role-policy ) Deploying Lambda function ( aws lambda create-function ) Invoking the lambda function ( aws lambda invoke )  Our lambda function will return a account detail json as below.\n{\u0026ldquo;id\u0026rdquo;: \u0026ldquo;23090\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;current\u0026rdquo;, \u0026ldquo;accountNum\u0026rdquo;: \u0026ldquo;45387954\u0026rdquo;, \u0026ldquo;sortCode\u0026rdquo;: \u0026ldquo;20-98-99\u0026rdquo;}\nLet\u0026rsquo;s start step by step\nCoding lambda function in GO Let\u0026rsquo;s create our project repo named \u0026ldquo;accounts\u0026rdquo;.\nNavigate to your GO workspace and create a folder named \u0026ldquo;accounts\u0026rdquo;\n$ mkdir accounts \u0026amp;\u0026amp; cd accounts\nCreating main.go Create main.go file\n$ touch main.go\nNow we will need to download aws go lambda package, so that we can use aws lambda libraries in our main.go code.\n$ go get github.com/aws/aws-lambda-go/lambda\nwhen we run this, go downloads libraries into your\n %GOPATH%/pkg\\mod\\github.com\\aws\\\n We can run $ go env GOPATH to check environment variable to find out where these aws lambda libraries is getting downloaded.\nOk, so by now we have an empty main.go and aws lambda libraries downloaded.\nLet\u0026rsquo;s create our lambda function.\n  Create a simple account struct with account details ( id, type, accountNum and sortcode)\n  Write a lambda handler which will initialize and return a new account object.\n  Call a lambda handler from main function.\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  package main import ( \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; ) // 1- creating account struct type account struct{ ID string `json:\u0026#34;id\u0026#34;` Type string `json:\u0026#34;type\u0026#34;` AccNum string `json:\u0026#34;accountNum\u0026#34;` SortCode string `json:\u0026#34;sortCode\u0026#34;` } // 2 - lambda handler - initializing and returning account object func showAccount() (*account, error) { account := \u0026amp;account{ ID: \u0026#34;23456\u0026#34;, Type: \u0026#34;current\u0026#34;, AccNum: \u0026#34;45328976\u0026#34;, SortCode: \u0026#34;30-89-99\u0026#34;, } return account, nil } func main(){ // 3 - calling lambda handler \tlambda.Start(showAccount) }   Lambda Handlers In the above code, function showAccount is a lambda handler. There are various signature available for lambda handler.\n The one we are using is func() (TOut, error)\nTin and Tout parameters are object that can be marshalled or unmarshalled using Go encoding/json package.\n func ()\nfunc () error\nfunc (TIn) error\nfunc () (TOut, error)\nfunc (context.Context) error\nfunc (context.Context, TIn) error\nfunc (context.Context) (TOut, error)\nfunc (context.Context, TIn) (TOut, error)\nüîÜ Have a look at AWS Lambda function handler in Go and refer to section Valid handler signatures.\nCreating go.mod Running go mod init \u0026lt;module-name\u0026gt; and then run go mod tidy command to download the dependencies required by source file.\n$ go mod init github.com/neerajsidhaye/go-modules-examples\n$ go mod tidy\nWe will now have a go.mod file created with below contents\n1 2 3 4 5 6 7  module github.com/neerajsidhaye/go-modules-examples go 1.16 require github.com/aws/aws-lambda-go v1.26.0   Creating build executable and ZIP for AWS We will now create an executable for account package using go build.\n $ env GOOS=linux GOARCH=amd64 go build -o main D:/dev/go-workspace/accounts\n This command will create an executable with name main in the current directory. You can provide any path for executable.\n We have used env for setting up two environment variables (GOOS=linux and GOARCH=amd64). This instructs go compiler to create an executable compatible for linux OS and amd64 architecture. This is what we will be using while deploying on AWS.\n creating zip file of executable AWS requires lambda function to deploy in a zip file format. The executable must be in the root of the zip file.\nOn windows, zip file can be created in PowerShell with below command\ncompress-archive  \u0026lt;output.zip\u0026gt;\ncompress-archive main main.zip\nthis will create main.zip file which will have main executable at the root of zip.\nAWS CLI configuration Install CLI First you need to install AWS CLI. If you don\u0026rsquo;t have it, you can refer here for windows download instruction Installing, updating, and uninstalling the AWS CLI version 2 on Windows\n$ aws --version\nthis will tell, which version of aws cli you have. Example:-\n aws-cli/2.2.8 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\n Setup IAM user for the CLI to use We need to create a IAM user with Programmatic access permission for the CLI to use.\nRefer here for Creating IAM user\nFor testing purpose, we can create a IAM user and attach AdministratorAccess policy to this user.\nIn practice, we must always attach a more restrictive policy to a user.\nConfigure CLI We will need to configure CLI to use credentials of IAM user we just created.\nInput access-key-Id and secret-access-Key of the IAM user you have crated with admin policy attached.\n$ aws configure\rAWS Access Key ID [none]: \u0026lt;input access-key-ID\u0026gt;\rAWS Secret Access Key [none]: \u0026lt;input secret-access-key\u0026gt;\rDefault region name [none]: eu-west-2\rDefault output format [none]: json\rCreating IAM role for Lambda Function We will now have to create an IAM role which defines permission that our lambda function will have when it\u0026rsquo;s running.\nWe will create a role called lambda-accounts-executor and attach AWSLambdaBasicExecutionRole managed policy to it.\nThis role ( lambda-accounts-executor ) will now give our lambda function a basic permission it need to run.\nCreating a trust policy Create a file called, trust-policy.json\nThis trust policy will instruct AWS to allow lambda services to assume the lambda-accounts-executor role.\n1 2 3 4 5 6 7 8 9 10 11 12 13  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }   Create IAM Role Creating role \u0026ldquo;lambda-accounts-executor\u0026rdquo; with trust policy\n$ aws iam create-role --role-name lambda-accounts-executor --assume-role-policy-document file://D:/Tech/AWS/golambda-user/trust-policy.json\rWe will have result like below. We will need Arn value in next step while deploying the lambda function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  { \u0026#34;Role\u0026#34;: { \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;RoleName\u0026#34;: \u0026#34;lambda-accounts-executor\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;AROATR36XMNXTAIYYCOPR\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;account-id\u0026gt;:role/lambda-accounts-executor\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2021-09-04T23:42:28+00:00\u0026#34;, \u0026#34;AssumeRolePolicyDocument\u0026#34;: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } } }   Attach policy to the role We have role (lambda-accounts-executor) created and now we will need to specify the permission the role has. We can do this by attaching a policy to the role.\n$ aws iam attach-role-policy --role-name lambda-accounts-executor --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\rWe have used aws iam attach-role-policy command, passing in the ARN of AWSLambdaBasicExecutionRole permission policy.\nDeploy lambda function Now we are ready to deploy our lambda function.\nWe will use command aws lambda create-function and which takes following parameters\n \u0026ndash;function name - Name of the lambda function called within AWS\n  \u0026ndash;runtime - The runtime environment for the lambda function\n  \u0026ndash;role - The ARN of the role you want the lambda function to assume when it is running.\n  \u0026ndash;handler - The of the executable in the root of the zip file.\n  \u0026ndash;zip-file - Path to the zip file.\n $ aws lambda create-function --function-name accounts --runtime go1.x --role arn:aws:iam::\u0026lt;account-id\u0026gt;:role/lambda-accounts-executor --handler main --zip-file fileb://D:/dev/go-workspace/accounts/main.zip\rWe should see output like as below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  { \u0026#34;FunctionName\u0026#34;: \u0026#34;accounts\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:eu-west-2:\u0026lt;account-id\u0026gt;:function:accounts\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;go1.x\u0026#34;, \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;account-id\u0026gt;:role/lambda-accounts-executor\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;CodeSize\u0026#34;: 4301831, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Timeout\u0026#34;: 3, \u0026#34;MemorySize\u0026#34;: 128, \u0026#34;LastModified\u0026#34;: \u0026#34;2021-09-05T00:07:07.466+0000\u0026#34;, \u0026#34;CodeSha256\u0026#34;: \u0026#34;9S2jsLAgT0YW6QvqGe0PjNgAK0ClI4S5fY/+xrcMLII=\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;TracingConfig\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; }, \u0026#34;RevisionId\u0026#34;: \u0026#34;ed7f4a67-5ffb-4dbb-850a-089b4c98127b\u0026#34;, \u0026#34;State\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;LastUpdateStatus\u0026#34;: \u0026#34;Successful\u0026#34;, \u0026#34;PackageType\u0026#34;: \u0026#34;Zip\u0026#34; }   That\u0026rsquo;s all, our lambda function is deployed successfully.\nInvoking lambda function We can use command aws lambda invoke , which ask for output file name for the response.\n$ aws lambda invoke --function-name accounts output.json\r{\r\u0026quot;StatusCode\u0026quot;: 200,\r\u0026quot;ExecutedVersion\u0026quot;: \u0026quot;$LATEST\u0026quot;\r}\routput.json gets created and it\u0026rsquo;s content is as below. This is exactly what our showAccount function in main.go is returning.\n1 2 3  {\u0026#34;id\u0026#34;:\u0026#34;23456\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;current\u0026#34;,\u0026#34;accountNum\u0026#34;:\u0026#34;45328976\u0026#34;,\u0026#34;sortCode\u0026#34;:\u0026#34;30-89-99\u0026#34;}   ","description":"Create, Deploy and Invoke - AWS lambda function in GO.","id":5,"section":"posts","tags":["GO","AWS"],"title":"AWS Lambda function in GO","uri":"https://neerajsidhaye.gibhub.io/posts/go/aws/golambdafunction/"},{"content":"In this post, we will be deploying docker image of a GO application on AWS EC2 instance.\n This involves below steps\n Launching an EC2 instance\n  SSH to EC2\n  Installing Docker on EC2\n  Starting Docker service\n  Pulling image from docker hub\n  Running the docker image on EC2\n  Testing the application\n For this post, I have created docker image of a GO rest api and pushed to my docker hub repository. We will be pulling the same go app image from docker hub for deploying onto EC2.\nIf you want to know how to create docker image of a go application, please have look at this post - Creating docker image of a Go application\nLaunching an EC2 instance Once you launch your EC2 instance, you will have a key pair. We will use the key pair for SSH to EC2 in the next step.\nPlease go through this article to know about, how to launch an EC2 instance\nSSH to EC2 If you are using windows powershell, you need to make sure that owner of the keypair file is the user who is executing the command.\nIn linux or Mac, we change .pem file permission using he chmod but in windows chmod won\u0026rsquo;t work, so we will modify keypair file by manually updating the file by going to file properties-\u0026gt;security tab-\u0026gt;Advanced button-\u0026gt; and making sure that owner of the file is the user who is executing the ssh to EC2 command.\nClick for file permission settings \r\r\rOnce the keypair file permission is set, run below command to connect to EC2 from powershell on windows \u0026gt;= 10\nssh -i \u0026lt;path to keypair file of EC2 instance\u0026gt; ec2-user@\u0026lt;Public IPv4 address of EC2 instance\u0026gt;\nexample:- ssh -i c:\\firstEC2KeyPair.pem ec2-user@3.4.555.666\rOnce you are successfully connected to EC2, let\u0026rsquo;s continue with next steps.\nInstalling Docker on EC2 simply run the command\nsudo yum install -y docker\rOnce docker install is success, start the docker service.\nStarting Docker Service sudo service docker start\rNow, in order to run docker command without sudo, we will need to add ec2-user to the docker group. Please run the below command.\nsudo usermod -a -G docker ec2-user\rAfter this, please logout and login back again to EC2 instance so that your group membership is re-evaluated.\nWe will now verify if we are able to run docker commands without sudo.\ndocker info\rPulling Image from Docker hub We have go application docker image on docker hub. Will will now download it\ndocker pull bethecodewithyou/go-images:go-rest-v1.0\r Running the Docker Image on EC2 listing docker images\ndocker images\rrunning the docker image of go application\ndocker run -d -p 8080:7070 bethecodewithyou/go-images:go-rest-v1.0\rchecking the running container\ndocker ps\r Testing Application simply run the curl to call product api\ncurl localhost:8080/products | jq\r| jq in above command is just to format the json.\nyou can install the jq using command\nsudo yum install jq\n ","description":"Deploying GO application docker image on AWS EC2 instance.","id":6,"section":"posts","tags":["GO","Docker","AWS"],"title":"Deploying GO app docker image on AWS EC2","uri":"https://neerajsidhaye.gibhub.io/posts/go/aws/deployinggoapponaws/"},{"content":"What is GO Middleware? Go middleware are like filters which gets executed for performing pre and/or post processing of the request.\nmiddleware or the filters implementation is achieved through http handler wrappers.\nHttpHandler wrappers are the functions which takes http.Handler as input and returns http.Handler as output.\nDefining Middleware Here we have defined a simple log request middleware, which takes one argument and returns one argument and of the same type http.Handler\nfunc logReq(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\rlog.Println(\u0026quot;URL called -\u0026quot;, r.URL ,\u0026quot; ,before calling original handler\u0026quot;)\rnext.ServeHTTP(w, r) // calling original handler ( helloHandler)\rlog.Println(\u0026quot;After calling original handler\u0026quot;)\r})\r}\rMiddleware Execution In order for this logReq middleware to execute, we will attach our original helloHandler to logReq middleware.\nserveMux.Handle(\u0026quot;/hello\u0026quot;, logReq(helloHandler))\rAfter this, every request on \u0026ldquo;/hello\u0026rdquo; will execute middleware ( logReq) before original application handler ( helloHandler ).\nShow me the code A simple middleware used in this post can be found here on github repo.\n","description":"GO middleware as the name suggests, sits between Go webserver( to be precise \"router\") and application handler and it works like a filters, similar to what we have in J2EE application for pre and/or post processing of request. Let's hands on with Go Middleware...","id":7,"section":"posts","tags":["GO","middleware","GO Core"],"title":"GO Middleware - request response interceptors/filters","uri":"https://neerajsidhaye.gibhub.io/posts/go/core/filter/gomiddleware/"},{"content":"In this post, we will scan GO application code for security issues using gosec and will also integrate gosec with SonarCloud.\nWe will explore below topics\n What is gosec?\n  Installing gosec\n  Running security checks for a GO code\n  Analyzing gosec reports\n  SonarCloud Integration with gosec\n  Companies using gosec\n What is gosec? gosec is a static code analyzer tool for inspecting go application code for security problems.\nstatic code analyzer meaning - analyze code without running the application.\nGosec currently has a set of 30 rules that map to the Common Weakness Enumeration (CWE) framework. Those rules help to secure your code by covering 8 of the 2019 CWE Top 25 Most Dangerous Software Errors.\ngosec github repo\nInstalling gosec gosec can be installed using go get or using CLI.\nIf you already have go installed then run below command to install gosec.\ngo get github.com/securego/gosec/cmd/gosec\rAfter running this command, you would see gosec executable available here $GO_PATH/bin/gosec.\nIf you don\u0026rsquo;t have GO installed, you can install gosec using CLI.\nPlease have a look at gosec CLI installation options\nRunning security checks for a GO project I will be using my existing go project github repo for running gosec security checks.\nYou could create a simple go project and execute gosec.\n Running gosec from project root folder\n gosec ./...\rThis will scan all the go files located in all the packages and sub packages from the root folder.\n  Expand Me - Running gosec.    Analyzing reports You can also specify security check result to be produced in a specific format and written to a file.\n json example\n gosec -fmt=json -out=gosecResult.json ./...\rThis will generate security scan result in the mentioned json file.\n  Expand Me - gosec report.    All the supported formats.\nSonarCloud Integration gosec security result can be easily integration with SonarCloud and then we analyse result on SonarCloud.\nConfiguring project in SonarCloud We will need to configure github repo in SonarCloud and for this, we will create new project in SonarCloud and link that project to github repo and branch.\n  Expand Me- Configuring github project in SonarCloud.    Adding sonar-project.properties to the project Few important properties\nsonar.projectKey = project key of new project which we just created on the SonarCloud.\nsonar.externalIssuesReportPaths = We will generate this report in sonar supported format by using gosec command.\nsonar.projectKey=gosec_go-rest-api\rsonar.organization=bethecodewithyou-github\rsonar.host.url=https://sonarcloud.io\rsonar.sources=.\rsonar.exclusions=**/*_test.go\rsonar.externalIssuesReportPaths=gosecReport.json\rGenerate gosec report for SonarCloud gosec -fmt=sonarqube -out=gosecReport.json ./... If you notice here, we are using sonarqube format and it is one of the supported formats in gosec.\nRunning the Sonar Scanner sonar-scanner.bat \\\r-D\u0026quot;sonar.organization=bethecodewithyou-github\u0026quot; \\\r-D\u0026quot;sonar.projectKey=gosec_go-rest-api\u0026quot; \\\r-D\u0026quot;sonar.sources=.\u0026quot; \\\r-D\u0026quot;sonar.host.url=https://sonarcloud.io\u0026quot;\rAfter running this command, the results should be available on your SonarCloud server shortly.\nYou can take a look here on - downloading and configuring Sonar Scanner.\nAnalyzing gosec report on SonarCloud Now, we will analyze gosec report directly on SonarCloud.\n Companies using gosec List of companies using gosec. Sourced from here.\n Gitlab CloudBees VMware Codacy Coinbase RedHat/OpenShift Guardalis 1Password PingCAP/tidb  ","description":"Scanning GO application code with gosec for security problems. gosec is static code analysis tool for finding security issues in go application. We will also integrate gosec with SonarCloud. Let's hands on...","id":8,"section":"posts","tags":["GO","Security","SonarCloud"],"title":"gosec - GO security scanner and gosec integration with SonarCloud","uri":"https://neerajsidhaye.gibhub.io/posts/go/security/goappsecuritychecker/"},{"content":"In this post, we will be creating an optimized docker image for GO application using multi stage build - using alpine image and then produce a small image with only binary in a scratch image. Let\u0026rsquo;s read further\u0026hellip;\nAssuming that you have got docker, git and GO installed on your machine so that you can build your GO app locally and then create a docker image.\nMulti Stage Build We will be creating a multi stage build.\n First stage -\u0026gt; We will use base image as golang:alpine Alpine Linux image to build our application.\n  Second stage -\u0026gt; We will use docker scratch image - zero byte image and then our build will contain binary executable built from first stage.\n GO Application I have got a GO application running locally, which is a simple Product API built using GorillaMux and exposing 5 endpoints.\nCode for Product API is available on my github repo - GO Product API\nGET /products/\nPOST /products/\nPUT /products/{id}\nPATCH /products/{id}\nDELETE /products/{id}\nLet\u0026rsquo;s quickly build and test app locally\n1 2 3  go build product.exe   Please click on below animated gif to get better view üòé\nCreating Dockerfile Let\u0026rsquo;s create docker file with multistage build.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  FROMgolang:alpine AS builderENV GO111MODULE=on \\  CGO_ENABLED=0 \\  GOOS=linux \\  GOARCH=amd64 # move to working directory buildWORKDIR/build# copy and download dependencices with go.modCOPY go.mod .COPY go.sum .RUN go mod download# COPY the source code from current dir to working dir into the containerCOPY . .# build the applicationRUN go build -o main ./cmd/app/product/# move to /dist directory as the place for resulting binary folderWORKDIR/dist# copy binary from bulid to main folderRUN cp /build/main .# Stage 2 - building a small image from Scratch ###FROMscratch# copy the prebuilt binary from the previous stageCOPY --from=builder /dist/main /# Expose the app portEXPOSE7070# command to runENTRYPOINT [ \u0026#34;/main\u0026#34; ]  Creating Docker image I have got docker installed on OracleVM virtual box on my Windows 10 machine.\nMy setup looks like this. Please expand the section Docker setup on Oracle VM\n  Expand Me- Docker setup on Oracle VM  Below is my local Windows 10 machine running docker version command.\n  Ok, let\u0026rsquo;s build our docker image.\n1  docker build -t go-rest-gorilla-docker .   Optimized image - it is just 8.02 MB üëã\n  Image without scratch - Now, if we modify our docker file and build image without scratch, then our docker image size would be 353 MB üò°\n Running Docker image üèÉ Ok, let\u0026rsquo;s run our docker image and test the application.\n1  docker run -d -p 8080:7070 go-rest-gorilla-docker  We are now running our docker image, mapped port 8080 to our exposed port 7070.\nIf you notice, I use IP address here not localhost because, my docker is running on OracleVM box which has got it\u0026rsquo;s IP address configured as 192.168.99.100\nCode Complete code is available on my github repo here GO Rest Gorilla - Product API\n","description":"Creating optimized docker image for GO application using multi stage build - using alpine image and then produce a small image with only binary in a scratch image. Let's read further...","id":11,"section":"posts","tags":["GO","Docker"],"title":"Dockerizing GO applications","uri":"https://neerajsidhaye.gibhub.io/posts/go/docker/dockerizegoapp/"},{"content":"In this post, I will show you, how simple and fast you can write automation tests for your API using ZeroCode framework\nBefore we start, let\u0026rsquo;s quickly read about Test Flavours - test types and where does ZeroCode framework fits in.\nTest Flavours   Expand - Test Flavours   Unit Tests This covers smallest piece of code which validates whether targeted code block works as expected with varieties of inputs. We use JUNIT and other mocking frameworks like Mockito etc as required.\n  Integration Tests This verifies communication paths and interaction between components and used to detect interface defects.\nFor Example:- If our application has got 3 microservices which might be consuming other external services or databases. Integration tests make sure that our application API\u0026rsquo;s are communicating well those external services as per agreed contacts.\n  Component Tests This is where we write test which cover various layers within an API as a whole component. We don\u0026rsquo;t go outside of our api boundary.\nFor example:- In this we write test for our Controllers and then the test flow goes through service layers, repository layers, gateway layers. Here we mock the api response of external api\u0026rsquo;s, because we are testing our own API component and not the external boundaries.\n  Contract Tests Verify the contracts with external boundary systems and making sure that it meets the contact expected by the consuming services.\nFor example:- Our application api\u0026rsquo;s are consuming external services and the contract with those external services meets the functionality.\n  End to end Tests As the name suggests, this verifies that the system works as expected as a whole entity from end to end - covering all internal and external components together.\n Ok, I think that pretty much covers various test flavours.\nZeroCode framework fits perfectly well for\nüëâ Integration Tests, üëâ Component Tests, üëâ Contract Tests and üëâ End to End Tets.\r  Alright cool, so if you have read about Test Flavours, then let\u0026rsquo;s get into code and find out how simple and quickly we can write tests with ZeroCode framework.\nüí• ZeroCode tests can be written for API's exposed in any language (JAVA, GO, DOT NET etc ).\rAs long the API is exposed over http, we are good to write test using ZeroCode.\r Product API We have got our Product API running locally with below end points and we will be writing ZeroCode tests for this Product API.\n  GET /products Returns list of products\n  POST /products Creates a new product\n  PUT /products/{id} Updates a product for a given product id\n  PATCH /products/{id}\nPartial updates to product for a given id\n  DELETE /products/{id}\nDeletes a product for a given product id\n  Product API running locally I have product api running on http://localhost:7070 and and it\u0026rsquo;s written in GO using Gorilla Mux library.\n  Expand - Product API  Please just click on below gif to get better view üòé\n  ZeroCode tests Steps to write ZeroCode tests.\nStep 1 - Generate test project using ArcheType Archetype is a fastest way to generate project skeleton and we can add boiler plate code with it.\nCreate a new folder ‚û°Ô∏è cd to that folder ‚û°Ô∏è copy below archetype command and just run it.\nYou should see a project ready for you with all the boiler plate code.\n mvn archetype:generate \\\r-DarchetypeGroupId=org.jsmart \\\r-DarchetypeArtifactId=zerocode-maven-archetype \\\r-DgroupId=com.ns \\\r-DartifactId=product-api-tests \\\r-Dversion=1.0.0-SNAPSHOT\rArchType - see in Action   Expand - ArchType In Action  Please just click on below gif to get better view üòé\n  Step 2 - Writing Tests Ok, Cool. We have got our project ready with dummy tests.\nNow let\u0026rsquo;s update config and tests\n  update hostconfig_ci.properties\nOpen hostconfig_ci.properties file and add url of our Product api against the key web.application.endpoint.host. I have added url of locally running Product API.\nweb.application.endpoint.host=http://localhost:7070\n #Continuous Integration Context\r# Web Server host and port\rweb.application.endpoint.host=http://localhost:7070\r# Web Service Port; Leave it blank in case it is default port i.e. 80 or 443 etc\rweb.application.endpoint.port=\r# Web Service context; Leave it blank in case you do not have a common context\rweb.application.endpoint.context=\r  Write tests cases using JSON\nWriting test for GET end point.\nFor all other endpoints tests, please have a look at the section\nWriting All Tests in Action below.\nGET /products\nOpen get_api_200.json and update below as per your api functionality.\nFor Product API, I have updated below.\n-\u0026gt; name - \u0026ldquo;get_product_detail\u0026rdquo;. It\u0026rsquo;s a step name to tell, what api we are testing.\n-\u0026gt; url - /products\n-\u0026gt; request body - left it blank here for GET request in this case\n-\u0026gt; verify status - It\u0026rsquo;s a assertion we are doing here. Verifying 200 status code.\n-\u0026gt; verify response body - Asserting that response body should have not-null id.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  { \u0026#34;scenarioName\u0026#34;: \u0026#34;Validate the GET api\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;get_product_details\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;/products\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;request\u0026#34;: { }, \u0026#34;verify\u0026#34;: { \u0026#34;status\u0026#34;: 200, \u0026#34;body\u0026#34;:[ { \u0026#34;id\u0026#34; : \u0026#34;$IS.NOTNULL\u0026#34; } ] } } ] }     And that\u0026rsquo;s all, Our test case is ready for GET PRODUCT details.\n  Expand Me- Writing all Tests in Action  Please just click on below gif to get better view üòé\n  That\u0026rsquo;s all! We are just done with writing out tests. It just really few mins!!\nStep 3 - Running the tests Let\u0026rsquo;s run our tests. We can run test directly from IDE and from command line and from Jenkins jobs. üòé\n For integration with CI/CD pipeline, we can just create mvn or gradle tasks for our tests and configure the task in the jenkins pipeline.\n   Expand Me- Running Tests from IDE  Please just click on below gif to get better view üòé\n  Running Individual Test from command line\nmvn -Dtest=MyGetApiTest test\rRunning TestSuite from command line\nmvn -Dtest=MyApiSuite test\rCreating gradle task\ntask runProductAPITestSuite ( type : Test ) {\rdelete \u0026quot;/target/\u0026quot;\rsystemProperty 'zerocode.junit', 'gen-smart-charts-csv-reports'\rinclude 'com/ns/MyApiSuite.class\r}\rRunning the gradle task - runProductAPITestSuite\ngradle runProductAPITestSuite\rTest Reports ZeroCode generates interactive test reports.\nJust build the test project and it will run all the tests and create a report folder which will have detailed report.\nmvn clean install\r  Expand Me- Test Reports  Please just click on below gif to get better view üòé\n  I hope you like this post. Please do share your comments.\n","description":"API Test Automation should be very simple, involve less learning curve, easy to collaborate, ease of integration with CI/CD pipeline and provide a good test reports. Developer and Testers both should be able to contribute in writing tests and most importantly, they should be focusing on writing quality test cases and not wasting anytime on learning the testing tool itself. ZeroCode open source framework is a perfect fit! Let's read on...","id":12,"section":"posts","tags":["API Test Automation"],"title":"API Test Automation - in minutes!!","uri":"https://neerajsidhaye.gibhub.io/posts/api/test/zerocodearchtype/"},{"content":"I am sure, how to structure a Go Project, would have been most obvious question which everybody have thought through and it is very obvious. Specially after we write some basic hello world which has only main.go or after doing workouts in Go Playground\nAs we step up and write more code which involve various layering, then it becomes very essential to organize the code, so that:-\n easy to understand and maintain each package purpose becomes self explanatory by it\u0026rsquo;s name reduce interdependencies in the code increase code reusability ease of collaboration  Well, you can think of more points, but the fact is, it is very important to follow a basic template which is set a as standard by various GO projects ( GO doesn\u0026rsquo;t provide any official docs on project structure as such) and later on top of that, one can evolve their own structure as needed.\nThis is what, I have followed so far for my basic applications in GO. I have not hit that complexity so far, as I have just started learning and writing basic applications in GO.\n/cmd/app/\u0026laquo;your app name\u0026raquo; So, on the root of your project, you will have cmd/app/\u0026laquo;your app name\u0026raquo;\nfor example If you are building product API, then could you below\n \u0026laquo;project-root\u0026raquo;/cmd/app/product/main.go\n This folder, will always have only one file, and that\u0026rsquo;s your starting point of project - main.go\n/internal As the name suggest, all files and sub folders inside internal, will be private your your application and will not be shared externally.\nfor example - you can further define sub-folders like data and handlers\n /internal/data/product.go\n/internal/handlers/product.go\n üîî internal is official: it‚Äôs the only directory named in Go‚Äôs documentation and has special compiler treatment\nüîî The other important point to note here is - in GO, you can have files with exact same name as long as they are in different package. Also, in GO, it is NOT a good practice to use naming like ProductHandlers.go. As the product.go file is defined in the handler package, it is implicit that it is handler. ‚≠ê\n/pkg You will have go files here, which are ok to be used by external applications. Other project will use these files from pkg. Mostly your public API code goes here in the pkg\n/static You could have static folder which contains static files used by your project. If files are internal to your app, then you can very well move this static folder inside internal.\nYou could have a look at very basic project structure which I have used in my repo in Go REST using Gorilla Mux\nI would also recommend to have a look at GO standard project layout\n","description":"What project structure to follow for GO projects? How to organize internal and external packages, so that things get simple and easy to collaborate. Lets read on...","id":13,"section":"posts","tags":["GO starter"],"title":"Go - Project Structure","uri":"https://neerajsidhaye.gibhub.io/posts/go/starter/goprojectlayout/"},{"content":"In this post, I will try to put together notes around GO service and REST API.\n The complete source code is available on my github repo.I have created 3 branches, each branch representing different flavour of writing service.\nGO api source code\nOk, let\u0026rsquo;s go ahead\u0026hellip;\nGO - WebService GO package net/http does the job for us. It stars the sever and ready to receive request at /hello with http.HandleFun\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; ) // a simple http web service which is exposing /hello end point on port 7070. func main() { http.HandleFunc(\u0026#34;/hello\u0026#34;, func(rw http.ResponseWriter, r *http.Request) { fmt.Println(\u0026#34;hello GO http web service\u0026#34;) data, err := ioutil.ReadAll(r.Body) // r.body - reads the request body \tif err != nil { http.Error(rw, \u0026#34;error occurred\u0026#34;, http.StatusInternalServerError) return } fmt.Fprintf(rw, \u0026#34;HELLO %s\\n\u0026#34;, data) }) // http webservice will be listening on any ip address and on port 7070. \thttp.ListenAndServe(\u0026#34;:7070\u0026#34;, nil) }   I have written another web service example, where I have created a Handler and ServeMux and then registering handler to ServeMux.\nYou can have a look at the code here - getProduct service\nGo - REST API GO REST API using standard go libraries. If you see the code of below product handler, you can see I have API with methods handling GET, POST and PUT.\nThe important point you would note here is, you will have to write a lot of code to parse URI params ( in this example) when you use go standard libraries.\nSame way, there may be other scenarios when you have to keep writing code for basic boiler plate processing.\nYou can see the quick demo here\nClick for Demo \r\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  //handler is serving httpRequest. Returning product response JSON. func (p *Product) ServeHTTP(rw http.ResponseWriter, r *http.Request) { if r.Method == http.MethodGet { p.l.Println(\u0026#34;handling GET\u0026#34;) p.getProducts(rw, r) return } if r.Method == http.MethodPost { p.l.Println(\u0026#34;handling POST\u0026#34;) p.addProduct(rw, r) return } if r.Method == http.MethodPut { p.l.Println(\u0026#34;handling PUT\u0026#34;) regex := regexp.MustCompile(`/([0-9]+)`) g := regex.FindAllStringSubmatch(r.URL.Path, -1) p.l.Printf(\u0026#34;regex g group %q\\n\u0026#34;, g) // if true, means there are more than one id passed in the URI. \tif len(g)!=1 { p.l.Println(\u0026#34;invalid product id in the URI\u0026#34;) http.Error(rw, \u0026#34;Invalid Request URI\u0026#34;, http.StatusBadRequest) return } if len(g[0]) \u0026gt; 2 { http.Error(rw, \u0026#34;Invalid URI\u0026#34;, http.StatusBadRequest) return } productID := g[0][1] idString, err := strconv.Atoi(productID) if err!=nil { http.Error(rw, \u0026#34;Invalid id value in URI\u0026#34;, http.StatusBadRequest) return } p.l.Println(\u0026#34;updating product for ID -\u0026gt;\u0026#34;, idString) p.updateProduct(idString, rw, r) return } // For any other methods, we are returning method not allowed \trw.WriteHeader(http.StatusMethodNotAllowed) }   GO - REST API using Gorilla Mux Include Gorilla mux dependency to go.mod.\nGo mod is like gradle or maven where we include dependencies.\n1 2 3  require ( github.com/gorilla/mux v1.8.0 )   Include the package gorilla mux to your go file.\n1 2 3  import ( \u0026#34;github.com/gorilla/mux\u0026#34; )   And now we will just create our servMux from gorilla Mux and register our product handler with various endpoints for handling GET, POST, PUT, PATCH and DELETE.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  servMux := mux.NewRouter() // registers product handler methods to serve request on api end points with specific http methods. \tgetHandler := servMux.Methods(http.MethodGet).Subrouter() getHandler.HandleFunc(\u0026#34;/products\u0026#34;, productHandler.GetProducts) postHandler := servMux.Methods(http.MethodPost).Subrouter() postHandler.HandleFunc(\u0026#34;/products\u0026#34;, productHandler.AddProduct) putHandler := servMux.Methods(http.MethodPut).Subrouter() putHandler.HandleFunc(\u0026#34;/products/{id:[0-9]+}\u0026#34;, productHandler.UpdateProduct) patchHandler := servMux.Methods(http.MethodPatch).Subrouter() patchHandler.HandleFunc(\u0026#34;/products/{id:[0-9]+}\u0026#34;, productHandler.UpdateProductAttribute) deleteHandler := servMux.Methods(http.MethodDelete).Subrouter() deleteHandler.HandleFunc(\u0026#34;/products/{id:[0-9]+}\u0026#34;, productHandler.DeleteProduct)   If you see here, it becomes so easy to parse the UI params using gorilla mux lib\n1 2 3 4 5 6 7  //UpdateProduct : updating a product func (p *Product) UpdateProduct(rw http.ResponseWriter, r *http.Request) { p.l.Println(\u0026#34;handling UPDATE\u0026#34;) uriParams := mux.Vars(r) id, err := strconv.Atoi(uriParams[\u0026#34;id\u0026#34;])   Take a look at the repo for details about how api is working and you can simply clone it and run locally.\nGO api source code\nHope you like this short post about GO api..\n","description":"Writing API in Go with various flavours. Simple Go http web service, GO REST API using Go standard libraries and Go REST API using Gorilla Mux.","id":14,"section":"posts","tags":["GO","REST API"],"title":"Go - Building API","uri":"https://neerajsidhaye.gibhub.io/posts/go/api/gorestapi/"},{"content":"I have put together my notes around some of the basic microservices design principles which we must always consider while designing a microservice.\nDesign principles for MicroServices 1. Develop and Deployed independently Each service should be developed and deployed independently. Deployment of one service should not impact other services and a each service should have it\u0026rsquo;s own code base.\nYou are doing wrong if\n You find a need to deploy services together You have one code base for multiple services You need to send notification to before you deploy a service  2. Data Ownership Service should have it\u0026rsquo;s own database or preferably set of tables that is manged by only a specific service. Should avoid scenario where multiple services are writing and reading from the same set of tables, any changes to table would require changes in all services.\nHaving each service has it\u0026rsquo;s own data ownership introduce loose coupling between service and database.\nkey point is, services should NOT have knowledge of each others underlying database.\nHaving own database for a services allows to choose right database technologies based on service functionality.\n3. Loosely coupled from all other services once you adhere to point 1 and 2, you have already initiated loose coupling but there are points to address on loose coupling:-\nminimal dependency on other services Communication with other services should be over exposed public interfaces( API, events etc ) and such interfaces or API should NOT expose internal details.\n4. Follow High Cohesion Methodology Closely related functionality must stay together in a single service. this minimizes intercommunication between services.\n5. Resilient service Single point of failure in system should NOT impact multiple services. if you have a service with independent data ownership, loose coupling, independent deployable artifact, it is a step towards resilient system!\nRemember - Total resilience in the face of all situations is NOT possible.Implement what is feasible in the short term and work to achieve greater resilience in stages.\n6. Shared Library Carefully watch out implications of shared library introduction to your services. You are doing something wrong when changes to shared library requires updates to all services simultaneously.\n7. Introduce Asynchronous Workers Very important design principle - introduce asynchronous workers to minimize impact on primary service API.\nExample -\u0026gt; A batch job can be introduced in a service as an asynchronous worker which is responsible to process high volume request, re-try mechanism for failed request etc.\nThis asynchronous worker provide following benefits:-\n Speed up the primary request path Spread load to asynchronous worker in case of high volume request Reduce error scenarios on primary API.  8. Service Versioning An API is never going to be completely stable. Change is inevitable!!\nIt\u0026rsquo;s always a best practice to version your service. Versioning can be added to header or in the service url. Following technique can be used to maintain service version:-\nThe URL has a major version number (v1), but the API has date based sub- versions which can be chosen using a custom HTTP request header. In this case, the major version provides structural stability of the API as a whole while the sub-versions accounts for smaller changes (field deprecations, endpoint changes, etc).\n","description":"Microservices at it's core is based on designing a bunch of small services based on specific business capabilities. Small service means, it should minimize complexity, should serve a focused purpose and should minimize inter-service communication. Importantly, each microservices should be built as a PRODUCT!\"","id":15,"section":"posts","tags":["API Design","MicroServices"],"title":"Microservices - Design Principle","uri":"https://neerajsidhaye.gibhub.io/posts/api/designprinciple/"},{"content":"This post describes basics of\n SSE concepts SSE use cases How does SSE work Message Formats SSE code on Client side and Server side SseEmitter connection keep alive time Auto Re-connect mechanism  SSE Concepts  Server Sent Events are the events ( data ) sent from server to the client over HTTP connection.\nThis connection is one directional connection from server to client. Meaning that, once the client connects to server, then there after server will send any real-time notifications generated on the server side, to the client.\nClient can be mobile app or browser based app, or any client that support HTTP Connection.\nSSE use cases You would have seen SSE use cases around you in day to day life\n Continuous update about train time notifications on the display panel on the platform. Continuous Rolling of Stock updates. Real time counter increment of your social media \u0026lsquo;likes\u0026rsquo; icon and could be more\u0026hellip;  How does SSE work Client initiates a connection to server over http, this can be done by client calling a rest end point on the server, in return, the response should have content-type header values as text/event-stream\nThis tells the client, that a connection is established and stream is opened for sending events from the server to the client.\nIn the browser you have a special object called, EventSource, that handles the connection and converts the responses into events.\nMessage-Formats SSE only supports text data. Meaning, server can only send text data to the client.\nBinary streaming, while possible, is inefficient with SSE. In that case, WebSocket would be good choice for binary data transfer.\nSSE code - Client side and Server side Client Side Code EventSource object is the core object supported by browser. To open a connection to the server, client will need to instantiate EventSource object.\n1  const eventSource = new EventSource(\u0026#39;http://localhost:8080/subscribe/\u0026#39;);   Browser sends this GET request with accept header text/event-stream.The response to this request, must contain header content-type with value text/event-stream and response must be encoded with UTF-8.\nTo process these events in the browser an application needs to register a istener for the message event.\nThe property data of the event object contains the message\n1 2 3 4  eventSource.onmessage = event =\u0026gt; { const msg = JSON.parse(event.data); // access your attributes from the msg. };   Client api supports certain events like open and error. Open event occurs as soon as 200 response is received by client for /subscribe GET call. Error event is received by client, when there is any network error or server terminates the connection.\nServer Side Code Http Response to the above GET request on /subscribe end point must contain the Content-Type header with the value text/event-stream.\nSpring Boot supports SSE by providing SseEmitter object. It was introduced in spring version 4.2 ( spring boot 1.3 ).\nCreate a spring boot application from start.spring.io and select web as dependency.\nYou can have a controller with rest end point GET with /subscribe allows client to establish connection.\nAnother rest end point POST with /event allows us to submit new events on the server. This POST with /events or similar end point, can be called from any other server side component to send real time notification.\nThis /event end point, will then send event to connected clients.\nEach client connection is represented with it\u0026rsquo;s own instance of SseEmitter.\nOne limitation with spring SSE is , it does not give you tools to manage these SseEmitter instances. So, for this example, I have used a list that stores SseEmitter objects and release objects on errors, completion or timeout scenarios.\nSseEmitter object is created as below\n1  SseEmitter emitter = new SseEmitter();   SseEmitter connection keep alive time By default, Spring Boot with the embedded Tomcat server keeps the SSE HTTP connection open for 30 seconds.We can override this 30 seconds via configurations.\nspring.mvc.async.request-timeout=50000\rthis entry will keep the HTTP connection open for 50 seconds. Alternatively, you can directly use SseEmitter constructor to pass this timeout value as below\nSseEmitter emitter = new SseEmitter(150_000L); //keep connection open for 150 seconds\rAuto Re-connect mechanism The nice thing about Server-Sent Events is that they have a built in re-connection feature. Meaning that, if the connection is dropped due to server error then client will automatically tries to re-connect after 3 seconds.\nThe browser tries to send reconnect requests forever until he gets a 200 HTTP response back.\nIt\u0026rsquo;s a browser feature to wait for 3 seconds and then automatically reconnect. This 3 seconds of time can be changed by the server by sending a new time value in the retry header attribute together with the message.\nA client can be told to stop reconnecting using the HTTP 204 No Content response code.\nDid you find this page helpful? Consider sharing it üôå ","description":"Server Sent Events - Concept, Use case, how SSE works, message formats, SSE code sample, SseEmitter connection keep alive time and Auto Reconnect mechanism","id":17,"section":"posts","tags":["Server Sent Events","SSE","EventSource"],"title":"Server Sent Events - Concepts","uri":"https://neerajsidhaye.gibhub.io/posts/sse/sse/"}]